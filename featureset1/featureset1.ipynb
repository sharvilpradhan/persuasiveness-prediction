{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import pandas_ml as pdml\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('hedgewords')\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.corpus import hedge_words\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"P2_Training_Dataset.csv\") \n",
    "\n",
    "post=data['text']\n",
    "ide=data['thread_id']\n",
    "label=data['delta']\n",
    "auth = data['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling the dataset to get rid of unimportant datapoints\n",
    "\n",
    "data = data.dropna(how='any', subset=['author_flair','text'])\n",
    "data.to_csv(r'C:\\Users\\Sharvil\\feature1\\sampled_training_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sampled_training_set.csv\") \n",
    "data = data.iloc[0:800]\n",
    "post=data['text']\n",
    "ide=data['thread_id']\n",
    "label=data['delta']\n",
    "auth = data['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataml = pdml.ModelFrame(data.to_dict(orient='list'))\n",
    "#sampler = dataml.imbalance.under_sampling.ClusterCentroids()\n",
    "#sampled = dataml.fit_sample(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "def sentiment(s2):\n",
    "    score = analyser.polarity_scores(s2)\n",
    "    listscore = list(score.values())\n",
    "    return listscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature - Sentiment analysis score\n",
    "op=0\n",
    "for i in range(1,ide.size):\n",
    "    if ide.loc[i]!=ide.loc[op]:\n",
    "        op=i\n",
    "        continue\n",
    "        \n",
    "    if auth.loc[op]==auth.loc[i]:\n",
    "        continue\n",
    "      \n",
    "    \n",
    "    s1=str(post.loc[op])\n",
    "    \n",
    "    f3=sentiment(s1)\n",
    "    data.loc[op, 'neg_score'] = f3[0]*100\n",
    "    data.loc[op, 'neut_score'] = f3[1]*100\n",
    "    data.loc[op, 'pos_score'] = f3[2]*100\n",
    "    data.loc[op, 'comp_score'] = f3[3]*100\n",
    "    \n",
    "    s2=str(post.loc[i])\n",
    "    lab=label.loc[i]\n",
    "    \n",
    "    \n",
    "    f3=sentiment(s2)\n",
    "    data.loc[i, 'neg_score'] = f3[0]*100\n",
    "    data.loc[i, 'neut_score'] = f3[1]*100\n",
    "    data.loc[i, 'pos_score'] = f3[2]*100\n",
    "    data.loc[i, 'comp_score'] = f3[3]*100\n",
    "    \n",
    "    \n",
    "data.to_csv(r'C:\\Users\\Sharvil\\feature1\\cleantrainingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(how='any',subset=['neg_score'])\n",
    "data.to_csv(r'C:\\Users\\Sharvil\\feature1\\cleantrainingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cleantrainingdata.csv\") \n",
    "post=data['text']\n",
    "ide=data['thread_id']\n",
    "label=data['delta']\n",
    "auth = data['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature - Length\n",
    "op=0;\n",
    "for i in range(1,ide.size):\n",
    "    if ide.loc[i]!=ide.loc[op]:\n",
    "        op=i\n",
    "        continue\n",
    "        \n",
    "    if auth.loc[op]==auth.loc[i]:\n",
    "        continue\n",
    "      \n",
    "    \n",
    "    s1=str(post.loc[op])\n",
    "    data.loc[op, 'length'] = len(s1)\n",
    "    \n",
    "    \n",
    "    s2=str(post.loc[i])\n",
    "    lab=label.loc[i]\n",
    "    data.loc[i, 'length'] = len(s2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "data.to_csv(r'C:\\Users\\Sharvil\\feature1\\cleantrainingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(s1,s2):\n",
    "    s1_vec = nlp(s1)\n",
    "    s2_vec = nlp(s2)\n",
    "    sim =  s1_vec.similarity(s2_vec)\n",
    "    return sim*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "#Feature - Cosine Similarity\n",
    "op=0;\n",
    "for i in range(1,ide.size):\n",
    "    if ide.loc[i]!=ide.loc[op]:\n",
    "        op=i\n",
    "        continue\n",
    "        \n",
    "    if auth.loc[op]==auth.loc[i]:\n",
    "        continue\n",
    "      \n",
    "    \n",
    "    s1=str(post.loc[op])\n",
    "    f2=cosine(s1,s1)\n",
    "    data.loc[op, 'similarity'] = f2\n",
    "    \n",
    "    \n",
    "    s2=str(post.loc[i])\n",
    "    lab=label.loc[i]\n",
    "    f2=cosine(s1,s2)\n",
    "    data.loc[i, 'similarity'] = f2\n",
    "    \n",
    "    \n",
    "    \n",
    "data.to_csv(r'C:\\Users\\Sharvil\\feature1\\cleantrainingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature - Count of Hedge words\n",
    "hdf = pd.read_csv(\"hedge_list.csv\")\n",
    "hlist=[]\n",
    "hlista= list(hdf['hedgewords'])\n",
    "hlist=str(hlista)\n",
    "#print(type(hlist))\n",
    "\n",
    "for i in range(1,ide.size):\n",
    "    count=0\n",
    "    p = str(post.loc[i])\n",
    "    wlist = p.split()\n",
    "    for x in wlist:\n",
    "        for y in hlist:\n",
    "            if x==y:\n",
    "                count=count+1\n",
    "    data.loc[i, 'count_hedgewords']= count\n",
    "    \n",
    "\n",
    "    \n",
    "data.to_csv(r'C:\\Users\\Sharvil\\feature1\\cleantrainingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampling \n",
    "minority_num = len(data[data.delta==1])\n",
    "minority_indices = data[data.delta==1].index\n",
    "minority_sample = data.loc[minority_indices]\n",
    "\n",
    "majority_indices = data[data.delta==0].index\n",
    "random_indices = np.random.choice(majority_indices,minority_num, replace=False)\n",
    "majority_sample = data.loc[random_indices]\n",
    "\n",
    "merged_sample = pd.concat([minority_sample,majority_sample],ignore_index=True)\n",
    "#print(merged_sample.head())\n",
    "data = merged_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=[]\n",
    "ytrain = []\n",
    "\n",
    "l = data['length']\n",
    "sim = data['similarity']\n",
    "neg = data['neg_score']\n",
    "neut = data['neut_score']\n",
    "pos = data['pos_score']\n",
    "comp = data['comp_score']\n",
    "hed = data['count_hedgewords']\n",
    "post=data['text']\n",
    "ide=data['thread_id']\n",
    "label=data['delta']\n",
    "auth = data['author']\n",
    "\n",
    "for i in range(1, ide.size):\n",
    "    v = [l.loc[i],sim.loc[i],neg.loc[i],neut.loc[i],pos.loc[i],comp.loc[i],hed.loc[i]]\n",
    "    lab = label.loc[i]\n",
    "    xtrain.append(v)\n",
    "    ytrain.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain=np.array(xtrain).astype(np.int32)\n",
    "ytrain=np.array(ytrain).astype(np.int32)\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1, n_estimators=300, max_features='auto')\n",
    "rfc.fit(xtrain, ytrain)\n",
    "#print('Random Forest percentage score on 25% validation=',rfc.score(xtrain,ytrain)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING DATASET\n",
    "datat = pd.read_csv(\"P2_Testing_Dataset.csv\") \n",
    "postt=datat['text']\n",
    "idet=datat['thread_id']\n",
    "labelt=datat['delta']\n",
    "autht = datat['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling the dataset to get rid of unimportant datapoints\n",
    "\n",
    "datat = datat.dropna(how='any', subset=['author_flair','text'])\n",
    "datat.to_csv(r'C:\\Users\\Sharvil\\feature1\\sampled_testing_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "datat = pd.read_csv(\"sampled_testing_set.csv\") \n",
    "datat = datat.iloc[0:800]\n",
    "postt=datat['text']\n",
    "idet=datat['thread_id']\n",
    "labelt=datat['delta']\n",
    "autht = datat['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature - Sentiment analysis score\n",
    "opt=0\n",
    "for i in range(1,idet.size):\n",
    "    if idet.loc[i]!=idet.loc[opt]:\n",
    "        opt=i\n",
    "        continue\n",
    "        \n",
    "    if autht.loc[opt]==autht.loc[i]:\n",
    "        continue\n",
    "      \n",
    "    \n",
    "    s1t=str(postt.loc[opt])\n",
    "    \n",
    "    f3=sentiment(s1t)\n",
    "    datat.loc[opt, 'neg_score'] = f3[0]*100\n",
    "    datat.loc[opt, 'neut_score'] = f3[1]*100\n",
    "    datat.loc[opt, 'pos_score'] = f3[2]*100\n",
    "    datat.loc[opt, 'comp_score'] = f3[3]*100\n",
    "    \n",
    "    s2t=str(postt.loc[i])\n",
    "    labt=labelt.loc[i]\n",
    "    \n",
    "    \n",
    "    f3=sentiment(s2t)\n",
    "    datat.loc[i, 'neg_score'] = f3[0]*100\n",
    "    datat.loc[i, 'neut_score'] = f3[1]*100\n",
    "    datat.loc[i, 'pos_score'] = f3[2]*100\n",
    "    datat.loc[i, 'comp_score'] = f3[3]*100\n",
    "    \n",
    "    \n",
    "datat.to_csv(r'C:\\Users\\Sharvil\\feature1\\cleantestingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "datat = datat.dropna(how='any',subset=['neg_score'])\n",
    "datat.to_csv(r'C:\\Users\\Sharvil\\feature1\\cleantestingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "datat = pd.read_csv(\"cleantestingdata.csv\") \n",
    "postt=datat['text']\n",
    "idet=datat['thread_id']\n",
    "labelt=datat['delta']\n",
    "autht = datat['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature - Length\n",
    "opt=0\n",
    "for i in range(1,idet.size):\n",
    "    if idet.loc[i]!=idet.loc[opt]:\n",
    "        opt=i\n",
    "        continue\n",
    "        \n",
    "    if autht.loc[opt]==autht.loc[i]:\n",
    "        continue\n",
    "      \n",
    "    \n",
    "    s1t=str(postt.loc[opt])\n",
    "    datat.loc[opt, 'length'] = len(s1t)\n",
    "    \n",
    "    \n",
    "    s2t=str(postt.loc[i])\n",
    "    labt=labelt.loc[i]\n",
    "    datat.loc[i, 'length'] = len(s2t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "datat.to_csv(r'C:\\Users\\Sharvil\\feature1\\cleantestingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "#Feature - Cosine Similarity\n",
    "opt=0\n",
    "for i in range(1,idet.size):\n",
    "    if idet.loc[i]!=idet.loc[opt]:\n",
    "        opt=i\n",
    "        continue\n",
    "        \n",
    "    if autht.loc[opt]==autht.loc[i]:\n",
    "        continue\n",
    "      \n",
    "    \n",
    "    s1t=str(postt.loc[opt])\n",
    "    f2=cosine(s1t,s1t)\n",
    "    datat.loc[opt, 'similarity'] = f2\n",
    "    \n",
    "    \n",
    "    s2t=str(postt.loc[i])\n",
    "    labt=labelt.loc[i]\n",
    "    f2=cosine(s1t,s2t)\n",
    "    datat.loc[i, 'similarity'] = f2\n",
    "    \n",
    "    \n",
    "    \n",
    "datat.to_csv(r'C:\\Users\\Sharvil\\feature1\\cleantestingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature - Count of Hedge words\n",
    "hdf = pd.read_csv(\"hedge_list.csv\")\n",
    "hlist=[]\n",
    "hlista= list(hdf['hedgewords'])\n",
    "hlist=str(hlista)\n",
    "#print(type(hlist))\n",
    "\n",
    "for i in range(1,idet.size):\n",
    "    count=0\n",
    "    p = str(postt.loc[i])\n",
    "    wlist = p.split()\n",
    "    for x in wlist:\n",
    "        for y in hlist:\n",
    "            if x==y:\n",
    "                count=count+1\n",
    "    datat.loc[i, 'count_hedgewords']= count\n",
    "    \n",
    "\n",
    "    \n",
    "datat.to_csv(r'C:\\Users\\Sharvil\\feature1\\cleantestingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0  Unnamed: 0.1 thread_id comment_id reply_to  \\\n",
      "0            0             1    2tijn9    cnzd976   2tijn9   \n",
      "1           20            51    2tj4pv    cnzy3ou   2tj4pv   \n",
      "2           42           126    2tjv6i    cnzozux   2tjv6i   \n",
      "3           70           206    2tlyzs    co15kpv   2tlyzs   \n",
      "4           71           213    2tlyzs    co0mpip   2tlyzs   \n",
      "5           82           288    2tlyzs    co0bhvs  co092b2   \n",
      "6          102           339    2tlyzs    co0lgzu  co0icbg   \n",
      "7          118           396    2tlyzs    co0nli1  co0fdr2   \n",
      "8          119           402    2tn8qu    co0jf5s   2tn8qu   \n",
      "9          217           693    2tpl0x    co1i4k5   2tpl0x   \n",
      "10         220           705    2tpl0x    co1enmk  co1d15j   \n",
      "11         240           799    2tquws    co1i3vy   2tquws   \n",
      "12         345          1077    2tuaj2    co2fk1u   2tuaj2   \n",
      "13         571          1553    2tuej1    co2ownm  co2nzpw   \n",
      "14         574          1557    2tuej1    co2pqwp  co2nzpw   \n",
      "15         616          1641    2txr2i    co3bnw1   2txr2i   \n",
      "16         628          1672    2txwe7    co3ki9h   2txwe7   \n",
      "17         630          1676    2txwe7    co3hzuu   2txwe7   \n",
      "18         652          1751    2tz5od    co3mhs2   2tz5od   \n",
      "19         674          1782    2tz5od    co3nb98  co3mtj8   \n",
      "20         676          1792    2tz7yr    co3pq16  co3p1ht   \n",
      "21         688          1815    2u1s4k    co4efyx   2u1s4k   \n",
      "22         786          1994    2u2fpm    co4ivor   2u2fpm   \n",
      "23         167           503    2tnwvc    co15009  co11b1s   \n",
      "24         696          1830    2u1s4k    co5ewq4   2u1s4k   \n",
      "25         387          1176    2tuaj2    co45joh  co2s3m7   \n",
      "26          56           158    2tkeqg    cnzt03k   2tkeqg   \n",
      "27         108           356    2tlyzs    co1hci2  co1ennr   \n",
      "28         744          1939    2u2fpm    co4vkaf  co4tcvv   \n",
      "29         395          1215    2tuaj2    co2d1xt   2tuaj2   \n",
      "30         327          1017    2tuaj2    co2iqbm  co2dn4b   \n",
      "31         799          2016    2u2fpm    co4zt3a  co4zjs3   \n",
      "32         258           830    2tquws    co1hhrw   2tquws   \n",
      "33         279           890    2tuaj2    co31vjr  co2ik88   \n",
      "34         731          1912    2u2fpm    co53mtg  co4ztfq   \n",
      "35         211           662    2tpl0x    co17xv5  co17usy   \n",
      "36         442          1345    2tuaj2    co327r3  co31aq1   \n",
      "37         793          2006    2u2fpm    co4k7d9  co4jfyc   \n",
      "38         392          1211    2tuaj2    co3ia94   2tuaj2   \n",
      "39         112           366    2tlyzs    co0etvz  co0aqkb   \n",
      "40         703          1842    2u1s4k    co4hhbj   2u1s4k   \n",
      "41         601          1608    2twq3l    co3iqdr  co38ro8   \n",
      "42         184           548    2tnwvc    co1p83b   2tnwvc   \n",
      "43          98           332    2tlyzs    co0tju8  co0apxf   \n",
      "44         370          1142    2tuaj2    co2jxzq  co2gep9   \n",
      "45         536          1501    2tuej1    co2r1ow  co2qbwg   \n",
      "\n",
      "                                                 text               author  \\\n",
      "0   It sounds like you're very much a pragmatist, ...              sousuke   \n",
      "1   I agree with your primary point, but would lik...      PrometheusDarko   \n",
      "2   The oculus can be pretty awesome for immersion...           SirCabbage   \n",
      "3   Wouldn't a compromise work better? I understan...              chilari   \n",
      "4   For some websites, such as financial instituti...           kingpatzer   \n",
      "5   One problem I see is you've opened yourself up...         Pluckerpluck   \n",
      "6   Certainly.  What I'm trying to do is compare t...            cacheflow   \n",
      "7   It's not 4^(2000*27), though. a four word pass...              mkurdmi   \n",
      "8   I think it's a very simplistic idea that solvi...      IIIBlackhartIII   \n",
      "9   There are unique challenges and opportunities ...               Dain42   \n",
      "10  &gt;The reality is that gay children are going...                 vl99   \n",
      "11  I agree with the idea that some people are mor...         scottevil110   \n",
      "12  My parents have made this same request and I g...           ricebasket   \n",
      "13  You still haven't explained why you want the t...  sarcasmandsocialism   \n",
      "14  &gt;His contributions to science are not disco...           garnteller   \n",
      "15  First of all, <&quot;>good<&quot;> in music of...           Account115   \n",
      "16        https://www.youtube.com/watch?v=Qm6kl17HH9s               hauxir   \n",
      "17  I'm not sure about your local institutions, bu...           -Jamerican   \n",
      "18  &gt;  <&quot;>I'm 91, they just called 71 so I...            cacheflow   \n",
      "19  Efficiency isn't necessarily an equation that ...           ricebasket   \n",
      "20  &gt;I guess what I need is a justification for...               huadpe   \n",
      "21  Scientists, as a group, are not any more virtu...             genebeam   \n",
      "22  I'll give this a shot... ...the time before ag...           AliceHouse   \n",
      "23  In practice lead bullets do pass through peopl...     SparklingLimeade   \n",
      "24  I think your objectives would be much better s...           Account115   \n",
      "25  There's a difference between getting money fro...                AusIV   \n",
      "26  Well the 250 is meant to be based off most pop...          lukeyflukey   \n",
      "27  No, that's the whole point.  Here's an example...                ryani   \n",
      "28  &gt;But only because some people have a lot mo...              sllewgh   \n",
      "29  I disagree with parents that do that, but it's...           beer_demon   \n",
      "30  The idea of sleeping in the same room but diff...          k9centipede   \n",
      "31  &gt; Farming might be a viable alternative if ...            SJHillman   \n",
      "32  **Note:** Your thread has **not** been removed...        AutoModerator   \n",
      "33  &gt;A <&quot;>before you're ready<&quot;> preg...       InfiniteQuasar   \n",
      "34  It's actually pretty well studied that people'...          hacksoncode   \n",
      "35                                           k, great            riggorous   \n",
      "36  The point I'm making is that it doesn't matter...            ivorystar   \n",
      "37                             True. But this is CMV.           AliceHouse   \n",
      "38  When living under someone else's roof, you nee...               funchy   \n",
      "39  But if passwords allow for upper/lower and spe...             gaviidae   \n",
      "40  Who picks those scientists? How do you prevent...         silverionmox   \n",
      "41  &gt;It's immoral to imprison people for not ha...               huadpe   \n",
      "42  I think there are two issues that exist for at...             sunburnd   \n",
      "43  So use 5 or 6 words, it will still be easier t...                ryani   \n",
      "44  &gt; Snuggling is one of the best things that ...      soswinglifeaway   \n",
      "45  &gt;So a question isn't scientific unless the ...              ghotier   \n",
      "\n",
      "       timestamp author_flair  author_score  delta  neg_score  neut_score  \\\n",
      "0   1.422114e+09            1          34.0      1        2.7        78.1   \n",
      "1   1.422158e+09            1           2.0      1       15.2        76.1   \n",
      "2   1.422138e+09            2           9.0      1       13.8        72.4   \n",
      "3   1.422269e+09            9           2.0      1        8.2        79.5   \n",
      "4   1.422225e+09           10           2.0      1        3.6        84.2   \n",
      "5   1.422204e+09            2          18.0      1        8.5        88.7   \n",
      "6   1.422223e+09           42           2.0      1        2.5        91.6   \n",
      "7   1.422227e+09            1           1.0      1       10.3        89.7   \n",
      "8   1.422219e+09           27          19.0      1       11.1        77.8   \n",
      "9   1.422298e+09            1           8.0      1        6.9        80.4   \n",
      "10  1.422293e+09           14           3.0      1       19.2        75.4   \n",
      "11  1.422298e+09           36           2.0      1       15.0        69.7   \n",
      "12  1.422374e+09           16         230.0      1        7.7        75.8   \n",
      "13  1.422389e+09           20           3.0      1        1.4        93.8   \n",
      "14  1.422390e+09          107           1.0      1        4.4        86.7   \n",
      "15  1.422432e+09            1           9.0      1        2.1        86.6   \n",
      "16  1.422461e+09            2           5.0      1        0.0       100.0   \n",
      "17  1.422456e+09            2           1.0      1        4.4        71.6   \n",
      "18  1.422464e+09           42          14.0      1       10.1        89.9   \n",
      "19  1.422465e+09           16           1.0      1        9.8        80.8   \n",
      "20  1.422469e+09           81           6.0      1        6.0        83.6   \n",
      "21  1.422511e+09           11          13.0      1        9.0        78.6   \n",
      "22  1.422529e+09            5          -3.0      1       15.5        73.8   \n",
      "23  1.422266e+09            1           1.0      0        6.5        79.6   \n",
      "24  1.422592e+09            1           1.0      0        3.3        83.5   \n",
      "25  1.422494e+09            4           1.0      0        0.0        88.1   \n",
      "26  1.422146e+09            1          21.0      0        3.1        80.1   \n",
      "27  1.422297e+09            1           1.0      0        4.4        89.0   \n",
      "28  1.422558e+09            4           2.0      0        9.6        69.8   \n",
      "29  1.422369e+09           26           2.0      0       17.4        64.4   \n",
      "30  1.422379e+09            4           2.0      0       21.9        78.1   \n",
      "31  1.422565e+09            1           2.0      0        0.0       100.0   \n",
      "32  1.422297e+09            3           0.0      0        0.0        95.7   \n",
      "33  1.422410e+09            1           1.0      0        9.6        76.6   \n",
      "34  1.422571e+09           96           3.0      0        6.4        75.0   \n",
      "35  1.422279e+09           11           1.0      0        0.0        19.6   \n",
      "36  1.422411e+09            1           1.0      0        9.9        82.0   \n",
      "37  1.422535e+09            5          -2.0      0        0.0        67.8   \n",
      "38  1.422456e+09           16           2.0      0        3.2        79.8   \n",
      "39  1.422210e+09           20           3.0      0        0.0        87.4   \n",
      "40  1.422522e+09            1           1.0      0        0.0        78.0   \n",
      "41  1.422457e+09           81           1.0      0       24.9        70.3   \n",
      "42  1.422309e+09            5           1.0      0        0.8        92.3   \n",
      "43  1.422237e+09            1           1.0      0        8.5        85.1   \n",
      "44  1.422381e+09            1           4.0      0        2.5        72.5   \n",
      "45  1.422392e+09           15           1.0      0        3.5        93.8   \n",
      "\n",
      "    pos_score  comp_score  length  similarity  count_hedgewords  \n",
      "0        19.2       98.64  1199.0  100.000000               NaN  \n",
      "1         8.7      -92.74   856.0   93.487679             120.0  \n",
      "2        13.7      -82.95  2986.0  100.000000            1020.0  \n",
      "3        12.3       96.87  2686.0   94.056738             720.0  \n",
      "4        12.2       93.00  1470.0   86.081037             360.0  \n",
      "5         2.8      -85.27   975.0   95.399430             300.0  \n",
      "6         6.0       81.30  1847.0   94.685625             300.0  \n",
      "7         0.0      -47.67   184.0   89.717379              60.0  \n",
      "8        11.1      -79.97  4864.0  100.000000             780.0  \n",
      "9        12.7       99.18  4160.0   98.156798            1020.0  \n",
      "10        5.4      -99.73  2561.0   98.585092             660.0  \n",
      "11       15.3      -51.13  1482.0  100.000000             120.0  \n",
      "12       16.5       98.38  1730.0   98.091838             420.0  \n",
      "13        4.7       28.69   798.0   97.641348             180.0  \n",
      "14        8.9       96.62  2572.0   98.258919             960.0  \n",
      "15       11.2       92.66  1010.0  100.000000             300.0  \n",
      "16        0.0        0.00    43.0    0.000000               0.0  \n",
      "17       24.0       96.26   419.0   94.895718               0.0  \n",
      "18        0.0      -88.72   631.0   98.050715             180.0  \n",
      "19        9.4       -7.32   400.0   96.308248              60.0  \n",
      "20       10.3       75.27  1300.0  100.000000             420.0  \n",
      "21       12.4       27.49   524.0  100.000000             360.0  \n",
      "22       10.7      -88.14  1056.0   93.670196             240.0  \n",
      "23       13.9       30.45   193.0   92.807573              60.0  \n",
      "24       13.2       95.58  1493.0   98.378052             300.0  \n",
      "25       11.9       57.89   337.0   96.103530              60.0  \n",
      "26       16.8       74.24   213.0  100.000000             120.0  \n",
      "27        6.6       85.21  1683.0   95.013935             540.0  \n",
      "28       20.7       95.97   893.0   92.200892             120.0  \n",
      "29       18.2       37.59   365.0   95.397974               0.0  \n",
      "30        0.0      -78.36   125.0   91.931839               0.0  \n",
      "31        0.0        0.00   172.0   90.244941             120.0  \n",
      "32        4.3       39.39   703.0   82.536831              60.0  \n",
      "33       13.8       29.44   291.0   95.774824             120.0  \n",
      "34       18.6       96.49  1028.0   92.679784             180.0  \n",
      "35       80.4       62.49     8.0   61.592585               0.0  \n",
      "36        8.1      -73.83  2400.0   96.343116             600.0  \n",
      "37       32.2       22.63    22.0   80.530981               0.0  \n",
      "38       17.0       97.64  1064.0   96.362748               0.0  \n",
      "39       12.6       70.96   272.0   91.829766              60.0  \n",
      "40       22.0       45.15    95.0   92.745368               0.0  \n",
      "41        4.8      -98.66   744.0   98.414581             240.0  \n",
      "42        6.9       82.11   979.0   97.194328             240.0  \n",
      "43        6.4      -39.47   489.0   91.800319             120.0  \n",
      "44       25.0       96.94   501.0   97.173828             180.0  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45        2.7      -22.63  1546.0   98.782267             540.0  \n"
     ]
    }
   ],
   "source": [
    "#undersampling \n",
    "minority_numt = len(datat[datat.delta==1])\n",
    "minority_indicest = datat[datat.delta==1].index\n",
    "minority_samplet = datat.loc[minority_indicest]\n",
    "\n",
    "majority_indicest = datat[datat.delta==0].index\n",
    "random_indicest = np.random.choice(majority_indicest,minority_numt, replace=False)\n",
    "majority_samplet = datat.loc[random_indicest]\n",
    "\n",
    "merged_samplet = pd.concat([minority_samplet,majority_samplet],ignore_index=True)\n",
    "#print(merged_sample.head())\n",
    "datat = merged_samplet\n",
    "print(datat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[856.0, 93.48767908214589, 15.2, 76.1, 8.7, -92.74, 120.0], [2986.0, 100.0, 13.8, 72.39999999999998, 13.7, -82.95, 1020.0], [2686.0, 94.0567381848059, 8.200000000000001, 79.5, 12.3, 96.87, 720.0], [1470.0, 86.08103735163836, 3.6, 84.2, 12.2, 93.0, 360.0], [975.0, 95.39942982046624, 8.5, 88.7, 2.8000000000000003, -85.27, 300.0], [1847.0, 94.68562450299133, 2.5, 91.6, 6.0, 81.3, 300.0], [184.0, 89.71737946664128, 10.3, 89.7, 0.0, -47.67, 60.0], [4864.0, 100.0, 11.1, 77.8, 11.1, -79.97, 780.0], [4160.0, 98.15679831948442, 6.9, 80.4, 12.7, 99.18, 1020.0], [2561.0, 98.58509198924814, 19.2, 75.4, 5.4, -99.73, 660.0], [1482.0, 100.0, 15.0, 69.69999999999999, 15.3, -51.13, 120.0], [1730.0, 98.09183791253048, 7.7, 75.8, 16.5, 98.38, 420.0], [798.0, 97.6413481632323, 1.4, 93.8, 4.7, 28.69, 180.0], [2572.0, 98.25891936701022, 4.4, 86.7, 8.9, 96.62, 960.0], [1010.0, 100.0, 2.1, 86.6, 11.2, 92.66, 300.0], [43.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0], [419.0, 94.89571784507831, 4.4, 71.6, 24.0, 96.26, 0.0], [631.0, 98.05071547019918, 10.1, 89.9, 0.0, -88.72, 180.0], [400.0, 96.30824765673574, 9.8, 80.80000000000003, 9.4, -7.32, 60.0], [1300.0, 100.0, 6.0, 83.6, 10.3, 75.27000000000002, 420.0], [524.0, 100.0, 9.0, 78.60000000000002, 12.4, 27.49, 360.0], [1056.0, 93.67019621843853, 15.5, 73.8, 10.7, -88.14, 240.0], [193.0, 92.80757310292047, 6.5, 79.60000000000002, 13.900000000000002, 30.45, 60.0], [1493.0, 98.37805215285316, 3.3000000000000003, 83.5, 13.2, 95.58, 300.0], [337.0, 96.10352963518989, 0.0, 88.1, 11.9, 57.89, 60.0], [213.0, 100.0, 3.1, 80.10000000000002, 16.8, 74.24, 120.0], [1683.0, 95.01393491763864, 4.4, 89.0, 6.6000000000000005, 85.21, 540.0], [893.0, 92.20089237177935, 9.6, 69.8, 20.7, 95.97, 120.0], [365.0, 95.39797409205623, 17.4, 64.4, 18.2, 37.59, 0.0], [125.0, 91.93183852954955, 21.9, 78.10000000000002, 0.0, -78.36, 0.0], [172.0, 90.24494117305282, 0.0, 100.0, 0.0, 0.0, 120.0], [703.0, 82.53683084622779, 0.0, 95.7, 4.3, 39.39, 60.0], [291.0, 95.77482430012995, 9.6, 76.6, 13.8, 29.44, 120.0], [1028.0, 92.6797836501349, 6.4, 75.0, 18.6, 96.49, 180.0], [8.0, 61.5925848879178, 0.0, 19.6, 80.4, 62.49, 0.0], [2400.0, 96.34311626487295, 9.9, 82.0, 8.1, -73.83, 600.0], [22.0, 80.5309813453453, 0.0, 67.80000000000001, 32.2, 22.63, 0.0], [1064.0, 96.36274789964901, 3.2, 79.80000000000003, 17.0, 97.64, 0.0], [272.0, 91.82976581404355, 0.0, 87.4, 12.6, 70.96000000000002, 60.0], [95.0, 92.74536819829503, 0.0, 78.0, 22.0, 45.15, 0.0], [744.0, 98.41458117636087, 24.9, 70.3, 4.8, -98.66, 240.0], [979.0, 97.19432772866664, 0.8, 92.3, 6.9, 82.11, 240.0], [489.0, 91.80031851818507, 8.5, 85.1, 6.4, -39.47, 120.0], [501.0, 97.17382815332113, 2.5, 72.5, 25.0, 96.94, 180.0], [1546.0, 98.78226727814678, 3.5000000000000004, 93.8, 2.7, -22.63, 540.0]]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "xtest=[]\n",
    "ytest = []\n",
    "\n",
    "lt = datat['length']\n",
    "simt = datat['similarity']\n",
    "negt = datat['neg_score']\n",
    "neutt = datat['neut_score']\n",
    "post = datat['pos_score']\n",
    "compt = datat['comp_score']\n",
    "hedt = datat['count_hedgewords']\n",
    "postt=datat['text']\n",
    "idet=datat['thread_id']\n",
    "labelt=datat['delta']\n",
    "autht = datat['author']\n",
    "\n",
    "for i in range(1, idet.size):\n",
    "    vt = [lt.loc[i],simt.loc[i],negt.loc[i],neutt.loc[i],post.loc[i],compt.loc[i],hedt.loc[i]]\n",
    "    labt = labelt.loc[i]\n",
    "    xtest.append(vt)\n",
    "    ytest.append(labt)\n",
    "    \n",
    "print(xtest)\n",
    "print(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.44444444444444\n"
     ]
    }
   ],
   "source": [
    "ypred = rfc.predict(xtest)\n",
    "print(rfc.score(xtest,ytest)*100)\n",
    "\n",
    "#Save ypred in csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Confusion Matrix: \n",
      " [[16  7]\n",
      " [ 9 13]]\n",
      "\n",
      " Accuracy:  64.44444444444444\n",
      "\n",
      " Precision, Recall, F1-score: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.70      0.67        23\n",
      "           1       0.65      0.59      0.62        22\n",
      "\n",
      "   micro avg       0.64      0.64      0.64        45\n",
      "   macro avg       0.65      0.64      0.64        45\n",
      "weighted avg       0.64      0.64      0.64        45\n",
      "\n",
      "0.28712871287128705\n"
     ]
    }
   ],
   "source": [
    "#Performance Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "print('\\n Confusion Matrix: \\n', confusion_matrix(ytest,ypred,labels=[0,1]))\n",
    "print('\\n Accuracy: ', accuracy_score(ytest,ypred)*100)\n",
    "print('\\n Precision, Recall, F1-score: \\n',classification_report(ytest,ypred))\n",
    "s = cohen_kappa_score(ytest,ypred)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
